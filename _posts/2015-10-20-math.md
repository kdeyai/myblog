---
layout: post
title: Support Vector Machines
date: 2022-02-15 10:11:00-0400
description: Some insights on Support Vector Machines
tags: SVM machine-learning
categories: classical machine learning
---
In classical machine learning we often study about support Vector machines or SVM . Support vector machines was invented by vladimir Vapnik in 1979 is a supervised machine learning algorithm which is used extensively for classification and regression in various projects related to data analysis and research . In this post we will go deep and understand SVM in full details.

<h4>Basic mathematical idea about SVM</h4>

For ease of understanding we consider that our data is linearly seperable and have just two classes i.e. it is a linearly seperable two class classification technique .

<div class="row mt-2">
    <div class="col-sm-5 mt-2 mt-md-0">
        {% include figure.html path="assets/img/svmfig1.png" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm-5 mt-2 mt-md-0">
        {% include figure.html path="assets/img/svmseperate.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
[Fig 1]  The above figure represents two classes depicted with blue and green color along with four hyperplanes P1 to P4 .<br>
[Fig 2]  The figure represents the separating hyperplanes as well as the support vectors .
</div>

We also know that Logistic Regression is also a linear classifier whose decision boundary is a plane or a hyperplane, so the question is what makes SVM different from logistic Regression. From fig 1. it is visible that there can be multiple decision boundaries for the above classification task in other words there are multiple planes(P1 to P4) which can separate my data but which plane or decision boundary is the optimal one . This is one of the reason's why SVM is preferred over Logistic Regression when we have a linearly separable data .

So how does SVM handles this problem ? Well, SVM uses margin maximization to find the best hyperplane which seperates the data .The best hyperplane is the one which is equally farthest from both the classes than any other hyperplanes.

The equation of the plane can be represented as $$w^Tx_i+b= 0$$ and the axis parallel hyperplanes can be written as $$w^Tx_i+b= 1$$ and $$w^Tx_i+b=-1$$ respectively . The distance between the axis parallel hyperplanes is the margin which is represented as $$ d $$ . SVM as a classifier is all about maximising the distance $$ d $$ . In simple words , SVM finds the optimal $$w$$ and $$b$$ for which the distance of the separating hyperplanes is maximum .

Mathematically, the optimization problem can be written as follows:
\begin{equation}
\left( w^*,b^0 \right)=\max_{w,b}\frac{2}{\Vert w\Vert} \text{such that } y_i(w^Tx_i+b) \geq 1 \text{ } \forall x_i
\end{equation}
Here, $$x_i$$ represents each datapoint in the dataset and $$y_i$$ represents each class label . $$w^*$$ and $$b^0$$ denotes the optimal w and b respectively. For simplicity our assumption of two class classification corresponds to $$y_i[-1,1] $$ . 

Now the question arises that when to stop maximizing? We stop maximizing once our axis parallel hyperplane meets its first data point from both the classes. These points are called support vectors . The best part of SVM is only the support vectors are responsible to change the decision boundary (hyperplanes in this case) , the non support vectors have no role to play and hence if non support vectors are outliers then it doesn't hurt the decision boundary as like other algorithms .

This type of SVM is called the hard margin SVM as while deriving the optimization equation we assumed the data is linearly separable and the number of classes are two but in real scenario the data is scattered ,therefore hard margin SVM will have a tough time finding the optimal hyperplane .To solve this problem we use soft margin SVM .

<div class="row mt-2">
    <div class="col-sm-3 mt-2 mt-md-0">   
    </div>
    <div class="col-sm-5 mt-2 mt-md-0">
        {% include figure.html path="assets/img/svmseperateerrors.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
[Fig 3] The above figure represents two classes depicted with blue and green color along with four hyperplanes P1 to P4 .
</div>

You can clearly observe from fig3 that now the data is scattered and does not have a clear decision boundary . In this case we need to change the optimization problem which we had proposed in $$(1)$$. The optimal hyperplane will now make some errors (the points which fall opposite to there correct hyperplane .) The equation of the hyperplanes therefore will be $$w^Tx_i+b=1-\xi$$. 

 $$\xi$$ refers to the distance of the misclassified point form the correct hyperplane . Now as the equation of the hyperplanes have changed we need to make changes to our optimization problem $$(1)$$.

Our new optimization problem will look like :

\begin{equation}
\left( w^*,b^0 \right)=\min_{w,b}\frac{\Vert w\Vert}{2} + c.\frac{1}{n}\left( \sum_{k=1}^n \xi_i \right)^2\text{such that } y_i(w^Tx_i+b) \geq 1-\xi_i \text{ , }\forall \xi_i \geq 0
\end{equation}

$$C$$ is the hyperparameter , larger $$c$$ corresponds to overfit as we give more importance to  minimize the errors and lesser c corresponds to underfit as we give less importance to the errors .
This form of SVM called the Soft Margin SVM  can handle scattered data and not necessarily linearly separated data . Athough this form of SVM also has a lot of drawbacks which we will soon discuss.

<h4>Dual form of SVM</h4>

The above form of SVM is often called the primal form of SVM . The problem with the primal form of SVM is that the constraint $$y_i(w^Tx_i+b) \geq 1-\xi_i $$ is often difficult to compute and hence we prefer to go for the dual form of SVM . 

Let us take an example and understand :

primal problem:
 $$\min_{x} x^2$$  s.t  $$x \geq b$$

 moving the constraint to objective function often called as the langrangian method
 $$L(x,\alpha)=x^2-\alpha(x-b)$$ s.t. $$\alpha \geq 0$$

Dual problem:
$$\max_\alpha \min_x L(x,\alpha) $$ s.t $$\alpha \geq 0$$

Whenever we make such changes we need to ensure that the primal problem's optimal solution and the dual problem's optimal solution remains the same, in that case we call it strong duality.

Whenever strong duality holds then the following conditions called the KKT(Karush Kuhn and Tucker) conditions also hold as well for $$\alpha^*$$ and $$x^*$$ .


$$\Delta L(x^*,\alpha^*)=0$$\\
$$x* \geq b$$\\
$$\alpha^* \geq 0$$\\
$$\alpha^*(x^*-b) =0$$


Now the question arises that why do we even need dual of a problem . The reason for this conversion is that the primal problem minimizes  $${\Vert w\Vert}$$ which is a $$d$$ dimensional vector where $$d$$ corresponds to the number of features . Generally features are multicollinear and high dimensional dataset often makes the task of minimization cumbersome. Whereas Dual form of SVM generally minimizes $$\alpha$$ which is corresponds to number of datapoints $$n$$ . This conversion from primal to dual makes SVM super powerfull to handle even high dimensional datasets . What would happen if our dataset is huge ? To solve such a problem we have SVM SMO which we will discuss later .

Let us now try to convert our SVM primal problem to dual

Primal problem of SVM
\begin{equation}
\left( w^*,b^0 \right)=\min_{w,b}\frac{\Vert w\Vert}{2}\text{such that } y_i(w^Tx_i+b) \geq 1 \text{ } \forall x_i
\end{equation}

Dual problem of SVM
\begin{equation}
L\left(w,b,\alpha\right)=\frac{\Vert w\Vert}{2} -\sum_j\alpha_j \[ \( w^t.x_j+b\)y_j -1\]\text{such that } \alpha_j \geq 0 \text{ } \forall j
\end{equation}

\begin{equation}
\max_\alpha \min_{w,b} L(w,b,\alpha)
\end{equation}

\begin{equation}
\max_\alpha \sum_{i}\alpha_i - \frac{1}{2}\sum_{i,j}\alpha_i \alpha_{j} y_{j}y_{i}x_{i}^Tx_{j} \text{ such that } \sum_i\alpha_iy_i=0 \text{, }\alpha_i \geq 0
\end{equation}

The $$x_i^Tx_j$$ is called the kernel and in the next section we will dig deep into kernalization .

<h4>Kernalization</h4>

<div class="row mt-2">
    <div class="col-sm-3 mt-2 mt-md-0">   
    </div>
    <div class="col-sm-5 mt-2 mt-md-0">
        {% include figure.html path="assets/img/kernal1.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
[Fig 4]  The above figure represents two classes depicted with red and black color showing the difficulty to find optimal hyperplane in such a setting .
</div>
As you can see clearly from the above figure that the data cannot be separated easily with linear models and finding an optimal hyperplane to such data is difficult and not feasible .
We solve this problem using kernalization which internally does nothing but feature transformation .In this post we discuss about three types of Kernel : Polynomial , RBF Kernel .

<h5>Polynomial Kernel</h5>

\begin{equation}
\max_\alpha \sum_{i}\alpha_i - \frac{1}{2}\sum_{i,j}\alpha_i \alpha_{j} y_{j}y_{i}K(x_{i}^Tx_{j}) \text{ such that } \sum_i\alpha_iy_i=0 \text{, }\alpha_i \geq 0
\end{equation}

Here, $$K(x_i,x_j)$$ is the kernel function . $$x_i,x_j$$ corresponds to all the data points present in the data. According to the above figure let us assume we have 2-D data i.e we have two feature data not clearly separable. 
 
 
\begin{equation}
K(x_1,x_2)=(c+x_1^Tx_2)^d
\end{equation}

Here both of my $$x_1=<x_{11},x_{12}>$$ and $$x_2=<x_{21},x_{22}>$$ are two dimensional and hence we have taken $$d=2$$.
After applying the values if $$d=2,c=1$$ to the $$(8)$$ , we get the following equation .
\begin{equation}
K(x_i,x_j)=(1+x_1^Tx_2)^2
\end{equation}

\begin{equation}
K(x_i,x_j)=1+x_{11}^2x_{22}^2+2x_{11}x_{21}+2x_{12}x_{22}+2x_{11}x_{21}x_{12}x_{22}
\end{equation}

\begin{equation}
x_1^{`}=[1,x_{11}^2,x_{12}^2,\sqrt 2 x_{11},\sqrt 2x_{12},\sqrt x_{11}x_{12}]
\end{equation}

\begin{equation}
x_2^{`}=[1,x_{21}^2,x_{22}^2,\sqrt 2 x_{21},\sqrt 2x_{22},\sqrt x_{21}x_{22}]
\end{equation}

\begin{equation}
K(x_i,x_j)=(x_1^{'})^T (x_2^{'})
\end{equation}

Equations $$11$$ and $$12$$ shows that the vector space has been transformed from $$2D$$ to $$6D$$. So why we want such a transformation , it is because the data points which were not seperable in $$2D$$ will be seperable in some other dimension. The kernel function does nothing but finds a suitable dimension where the points will be separable . 

<h5>Radial Basis Function (RBF) kernel</h5>

We have seen the polynomial function which tries to find a dimension where your classes are linearly seperable . But when dimensions increase training models becomes difficult and lots of problems pop in , therefore researchers propose RBF kernel which can solve this problem without moving to a higher dimension .

\begin{equation}
K_{RBF}(x_i,x_j)=e^{\frac{-(\Vert x_1-x_2 \Vert)^2}{2 \sigma ^2}}
\end{equation}

So how does an Radial Basis Function try to find a suitable dimension . As you can see from the above equation RBF Kernel tries to find ratio of the distance between any two vectors and $$\sigma$$ , $$\sigma$$ here is a hyperparameter along with C. 
